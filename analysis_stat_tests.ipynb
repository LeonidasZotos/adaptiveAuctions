{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Analysis Notebook\n",
    "\n",
    "This notebook can be used to calculate statistical metrics for the data produced. \n",
    "*** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup <a class=\"anchor\" id=\"0\"></a>\n",
    "This section imports all files and sets up the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all pacakges\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using results: ['Random', 'E_Greedy_Exp_Decay']\n"
     ]
    }
   ],
   "source": [
    "# Choose which results to use\n",
    "results_to_use = [\"Random\", \"E_Greedy_Exp_Decay\"]  # If this is empty, all files in the folder will be used\n",
    "WARMUP_EPOCHS = 0 # Number of epochs to ignore.\n",
    "sim_colours = ['blue', 'green', 'red', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "\n",
    "\n",
    "all_results = os.listdir('results/')\n",
    "\n",
    "if results_to_use == [\"\"]:\n",
    "    results_to_use = all_results\n",
    "# remove DS_Store from the list\n",
    "if '.DS_Store' in results_to_use:\n",
    "    results_to_use.remove('.DS_Store')\n",
    "    \n",
    "print(\"Using results: \" + str(results_to_use))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary. Each key is a set of results. \n",
    "\n",
    "all_sim_results = {}\n",
    "\n",
    "for sim in results_to_use:\n",
    "    all_sim_results[sim] = {}\n",
    "    all_sim_results[sim]['configuration'] = pd.read_csv(\n",
    "        'results/' + sim + '/configuration.txt', header=None, delimiter=\":\")\n",
    "    types_of_results = []\n",
    "    # Get all the filenames in the folder, excluding the extension\n",
    "    for file in os.listdir('results/' + sim + '/exported_data/'):\n",
    "        if file.endswith(\".npy\"):\n",
    "            types_of_results.append(file[:-4])\n",
    "    # Create a dictionary for each type of result\n",
    "    for result in types_of_results:\n",
    "        all_sim_results[sim][result] = np.load(\n",
    "            'results/' + sim + '/exported_data/' + result + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Random\n",
      "                                    0                   1\n",
      "0                             command                 run\n",
      "1                       num_of_epochs               10000\n",
      "2                  num_of_simulations                  10\n",
      "3                           grid_size                   3\n",
      "4                      queue_capacity                  10\n",
      "5                     congestion_rate                0.07\n",
      "6                       with_hotspots                True\n",
      "7                           wage_time                  10\n",
      "8                      credit_balance                   5\n",
      "9           shared_auction_parameters                None\n",
      "10  adaptive_auction_action_selection              random\n",
      "11               bid_calculation_rule              linear\n",
      "12       adaptive_auction_update_rule       simple_bandit\n",
      "13             auction_episode_length                  10\n",
      "14   action_selection_hyperparameters                None\n",
      "15    adaptive_auction_discretization                  25\n",
      "16             only_winning_bid_moves                None\n",
      "17           intersection_reward_type   mixed_metric_rank\n",
      "18                  inact_rank_weight                 0.5\n",
      "19                    bid_rank_weight                 0.5\n",
      "20                       all_cars_bid                True\n",
      "21               shared_bid_generator                None\n",
      "22                 bidders_proportion     [1, 0, 0, 0, 1]\n",
      "23       bidders_urgency_distribution            gaussian\n",
      "24                     results_folder      results/Random\n",
      "25                         print_grid                None\n",
      "26                         sweep_mode                None\n",
      "27                            low_dpi                True\n",
      "Results: E_Greedy_Exp_Decay\n",
      "                                    0                            1\n",
      "0                             command                          run\n",
      "1                       num_of_epochs                        10000\n",
      "2                  num_of_simulations                           10\n",
      "3                           grid_size                            3\n",
      "4                      queue_capacity                           10\n",
      "5                     congestion_rate                         0.07\n",
      "6                       with_hotspots                         True\n",
      "7                           wage_time                           10\n",
      "8                      credit_balance                            5\n",
      "9           shared_auction_parameters                         None\n",
      "10  adaptive_auction_action_selection           e_greedy_exp_decay\n",
      "11               bid_calculation_rule                       linear\n",
      "12       adaptive_auction_update_rule                simple_bandit\n",
      "13             auction_episode_length                           10\n",
      "14   action_selection_hyperparameters                         None\n",
      "15    adaptive_auction_discretization                           25\n",
      "16             only_winning_bid_moves                         None\n",
      "17           intersection_reward_type            mixed_metric_rank\n",
      "18                  inact_rank_weight                          0.5\n",
      "19                    bid_rank_weight                          0.5\n",
      "20                       all_cars_bid                         True\n",
      "21               shared_bid_generator                         None\n",
      "22                 bidders_proportion              [1, 0, 0, 0, 1]\n",
      "23       bidders_urgency_distribution                     gaussian\n",
      "24                     results_folder   results/E_Greedy_Exp_Decay\n",
      "25                         print_grid                         None\n",
      "26                         sweep_mode                         None\n",
      "27                            low_dpi                         True\n"
     ]
    }
   ],
   "source": [
    "# Show all the all_sim_results in the notebook\n",
    "for set_of_results in results_to_use:\n",
    "    print(\"Results: \" + set_of_results)\n",
    "    print(all_sim_results[set_of_results]['configuration'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Average Total Number of Trips  <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "This t-test compares the average total number of trips between the two experiments, for all sims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=-2.3839727470307106, pvalue=0.02834310244952404, df=18.0)\n",
      "The average number of trips for the first two simulations are: 24763.7 and 24883.8\n",
      "The p-value is: 0.02834310244952404\n",
      "The difference is significant\n"
     ]
    }
   ],
   "source": [
    "all_average_number_of_trips = []\n",
    "for sim in results_to_use:\n",
    "    all_average_number_of_trips.append(\n",
    "        all_sim_results[sim]['stat_num_of_trips_per_simulation'])\n",
    "\n",
    "results = stats.ttest_ind(all_average_number_of_trips[0], all_average_number_of_trips[1])\n",
    "print(results)\n",
    "#present the results nicely\n",
    "print(\"The average number of trips for the first two simulations are: \" + str(np.mean(all_average_number_of_trips[0])) + \" and \" + str(np.mean(all_average_number_of_trips[1])))\n",
    "print(\"The p-value is: \" + str(results[1]))\n",
    "if results[1] < 0.05:\n",
    "    print(\"The difference is significant\")\n",
    "else:\n",
    "    print(\"The difference is not significant\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Average Congestion of central Intersection\n",
    "\n",
    "This t-test is between the 2 experiments, for all sims. It does not process individual epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=5.831636558031212, pvalue=1.5939058704940788e-05, df=18.0)\n",
      "The average congestion for the first two simulations are: 0.32815325 and 0.31273275\n",
      "The p-value is: 1.5939058704940788e-05\n",
      "The difference is significant\n"
     ]
    }
   ],
   "source": [
    "average_congestion_per_intersection = []\n",
    "for exp in results_to_use:\n",
    "    average_congestion_per_intersection.append(\n",
    "        all_sim_results[exp]['stat_average_congestion_per_intersection'])\n",
    "\n",
    "results = stats.ttest_ind(average_congestion_per_intersection[0][:,1,1], average_congestion_per_intersection[1][:,1,1])\n",
    "print(results)\n",
    "print(\"The average congestion for the first two simulations are: \" + str(np.mean(average_congestion_per_intersection[0][:,1,1])) + \" and \" + str(np.mean(average_congestion_per_intersection[1][:,1,1])))\n",
    "print(\"The p-value is: \" + str(results[1]))\n",
    "if results[1] < 0.05:\n",
    "    print(\"The difference is significant\")\n",
    "else:\n",
    "    print(\"The difference is not significant\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Average Time Waited <a id=\"3\"></a>\n",
    "\n",
    "agent, intersection and grid based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=20.30700598460725, pvalue=7.403600266153952e-14, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.2757655555555555 and 0.2379477777777778\n",
      "The p-value is: 7.403600266153952e-14\n",
      "TtestResult(statistic=18.338219578216414, pvalue=4.282144123588275e-13, df=18.0)\n",
      "The max time waited for the first two simulations are: 0.7119411111111111 and 0.6166555555555555\n",
      "The p-value is: 4.282144123588275e-13\n"
     ]
    }
   ],
   "source": [
    "######Agent based######\n",
    "# Average time\n",
    "average_time_agent = []\n",
    "for exp in results_to_use:\n",
    "    average_time_agent.append(\n",
    "        all_sim_results[exp]['stat_average_time_waited_per_simulation_agent'])\n",
    "    \n",
    "results = stats.ttest_ind(average_time_agent[0], average_time_agent[1])\n",
    "print(results)\n",
    "print(\"The average time waited for the first two simulations are: \" + str(np.mean(average_time_agent[0])) + \" and \" + str(np.mean(average_time_agent[1])))\n",
    "print(\"The p-value is: \" + str(results[1]))\n",
    "\n",
    "# Max time\n",
    "max_time_agent = []\n",
    "for exp in results_to_use:\n",
    "    max_time_agent.append(\n",
    "        all_sim_results[exp]['stat_max_time_waited_per_simulation_agent'])\n",
    "    \n",
    "results = stats.ttest_ind(max_time_agent[0], max_time_agent[1])\n",
    "print(results)\n",
    "print(\"The max time waited for the first two simulations are: \" + str(np.mean(max_time_agent[0])) + \" and \" + str(np.mean(max_time_agent[1])))\n",
    "print(\"The p-value is: \" + str(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection [ 0 ][ 0 ]\n",
      "TtestResult(statistic=-1.8347096462753625, pvalue=0.0831318127408832, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.0231125 and 0.026392500000000003\n",
      "The p-value is: 0.0831318127408832\n",
      "Intersection [ 0 ][ 1 ]\n",
      "TtestResult(statistic=1.3871492403803547, pvalue=0.182334090048716, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.24544499999999997 and 0.23470249999999998\n",
      "The p-value is: 0.182334090048716\n",
      "Intersection [ 0 ][ 2 ]\n",
      "TtestResult(statistic=-1.0479265580533594, pvalue=0.30854047577592786, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.023865 and 0.025095\n",
      "The p-value is: 0.30854047577592786\n",
      "Intersection [ 1 ][ 0 ]\n",
      "TtestResult(statistic=-1.6219612271316932, pvalue=0.12219610969865431, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.3023175 and 0.32062749999999995\n",
      "The p-value is: 0.12219610969865431\n",
      "Intersection [ 1 ][ 1 ]\n",
      "TtestResult(statistic=13.074391167469198, pvalue=1.2540022914574625e-10, df=18.0)\n",
      "The average time waited for the first two simulations are: 1.295135 and 0.9363949999999999\n",
      "The p-value is: 1.2540022914574625e-10\n",
      "Intersection [ 1 ][ 2 ]\n",
      "TtestResult(statistic=-0.4473638876782853, pvalue=0.6599491793198176, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.271255 and 0.2768325\n",
      "The p-value is: 0.6599491793198176\n",
      "Intersection [ 2 ][ 0 ]\n",
      "TtestResult(statistic=1.3982089085873888, pvalue=0.17904218353324747, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.0278425 and 0.024445\n",
      "The p-value is: 0.17904218353324747\n",
      "Intersection [ 2 ][ 1 ]\n",
      "TtestResult(statistic=-0.16572286328358193, pvalue=0.8702222129789164, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.2706125 and 0.27257750000000003\n",
      "The p-value is: 0.8702222129789164\n",
      "Intersection [ 2 ][ 2 ]\n",
      "TtestResult(statistic=-1.684652765993672, pvalue=0.10931511312862963, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.022305 and 0.024462499999999998\n",
      "The p-value is: 0.10931511312862963\n"
     ]
    }
   ],
   "source": [
    "######Intersection based - Average Time######\n",
    "\n",
    "average_time_intersection = []\n",
    "for exp in results_to_use:\n",
    "    average_time_intersection.append(\n",
    "        all_sim_results[exp]['stat_average_time_waited_per_intersection'])\n",
    "    \n",
    "for i in range(len(average_time_intersection[0][0])):\n",
    "    for j in range(len(average_time_intersection[0][0])):\n",
    "        print(\"Intersection [\", i, \"][\", j, \"]\")\n",
    "        results = stats.ttest_ind(average_time_intersection[0][:,i,j], average_time_intersection[1][:,i,j])\n",
    "        print(results)\n",
    "        print(\"The average time waited for the first two simulations are: \" + str(np.mean(average_time_intersection[0][:,i,j])) + \" and \" + str(np.mean(average_time_intersection[1][:,i,j])))\n",
    "        print(\"The p-value is: \" + str(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection [ 0 ][ 0 ]\n",
      "TtestResult(statistic=-1.9938529068505888, pvalue=0.06154416981666345, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.08226000000000001 and 0.09254000000000001\n",
      "The p-value is: 0.06154416981666345\n",
      "Intersection [ 0 ][ 1 ]\n",
      "TtestResult(statistic=1.4936859508034832, pvalue=0.15258049595304954, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.6872 and 0.6571400000000001\n",
      "The p-value is: 0.15258049595304954\n",
      "Intersection [ 0 ][ 2 ]\n",
      "TtestResult(statistic=-1.1259707556256784, pvalue=0.2749669076280165, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.08464999999999999 and 0.08861\n",
      "The p-value is: 0.2749669076280165\n",
      "Intersection [ 1 ][ 0 ]\n",
      "TtestResult(statistic=-1.3981283459616458, pvalue=0.17906598852197048, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.82746 and 0.8667400000000001\n",
      "The p-value is: 0.17906598852197048\n",
      "Intersection [ 1 ][ 1 ]\n",
      "TtestResult(statistic=12.702446384019048, pvalue=2.0088214702584773e-10, df=18.0)\n",
      "The average time waited for the first two simulations are: 3.0502100000000003 and 2.1580999999999997\n",
      "The p-value is: 2.0088214702584773e-10\n",
      "Intersection [ 1 ][ 2 ]\n",
      "TtestResult(statistic=-0.36424255053037813, pvalue=0.7199205464728329, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.7503900000000001 and 0.76188\n",
      "The p-value is: 0.7199205464728329\n",
      "Intersection [ 2 ][ 0 ]\n",
      "TtestResult(statistic=1.0605756044544956, pvalue=0.3029070841291798, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.09402999999999999 and 0.08665999999999999\n",
      "The p-value is: 0.3029070841291798\n",
      "Intersection [ 2 ][ 1 ]\n",
      "TtestResult(statistic=-0.003086032790808746, pvalue=0.9975716510527602, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.7511300000000001 and 0.75122\n",
      "The p-value is: 0.9975716510527602\n",
      "Intersection [ 2 ][ 2 ]\n",
      "TtestResult(statistic=-1.8406826858227756, pvalue=0.08221416064606135, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.08013999999999999 and 0.08701\n",
      "The p-value is: 0.08221416064606135\n"
     ]
    }
   ],
   "source": [
    "######Intersection based - Max Time######\n",
    "\n",
    "max_time_intersection = []\n",
    "for exp in results_to_use:\n",
    "    max_time_intersection.append(\n",
    "        all_sim_results[exp]['stat_max_time_waited_per_intersection'])\n",
    "    \n",
    "for i in range(len(max_time_intersection[0][0])):\n",
    "    for j in range(len(max_time_intersection[0][0])):\n",
    "        print(\"Intersection [\", i, \"][\", j, \"]\")\n",
    "        results = stats.ttest_ind(max_time_intersection[0][:,i,j], max_time_intersection[1][:,i,j])\n",
    "        print(results)\n",
    "        print(\"The average time waited for the first two simulations are: \" + str(np.mean(max_time_intersection[0][:,i,j])) + \" and \" + str(np.mean(max_time_intersection[1][:,i,j])))\n",
    "        print(\"The p-value is: \" + str(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE TIME WAITED\n",
      "TtestResult(statistic=20.30700598460726, pvalue=7.403600266153899e-14, df=18.0)\n",
      "The average time waited for the first two simulations are: 0.2757655555555556 and 0.2379477777777778\n",
      "The p-value is: 7.403600266153899e-14\n",
      "MAX TIME WAITED\n",
      "TtestResult(statistic=18.338219578216368, pvalue=4.2821441235884436e-13, df=18.0)\n",
      "The max time waited for the first two simulations are: 0.7119411111111111 and 0.6166555555555556\n",
      "The p-value is: 4.2821441235884436e-13\n"
     ]
    }
   ],
   "source": [
    "######Grid based######\n",
    "\n",
    "# Average time\n",
    "print(\"AVERAGE TIME WAITED\")\n",
    "average_time_grid = []\n",
    "for exp in results_to_use:\n",
    "    average_time_grid.append(\n",
    "        all_sim_results[exp]['stat_average_time_waited_grid'])\n",
    "\n",
    "means_1 = []\n",
    "for i in range(len(average_time_grid[0])):\n",
    "    means_1.append(np.mean(average_time_grid[0][i]))\n",
    "means_2 = []\n",
    "for i in range(len(average_time_grid[1])):\n",
    "    means_2.append(np.mean(average_time_grid[1][i]))\n",
    "\n",
    "\n",
    "results = stats.ttest_ind(means_1, means_2)\n",
    "print(results)\n",
    "print(\"The average time waited for the first two simulations are: \" + str(np.mean(means_1)) + \" and \" + str(np.mean(means_2)))\n",
    "print(\"The p-value is: \" + str(results[1]))\n",
    "\n",
    "\n",
    "# Max time\n",
    "print(\"MAX TIME WAITED\")\n",
    "max_time_grid = []\n",
    "for exp in results_to_use:\n",
    "    max_time_grid.append(\n",
    "        all_sim_results[exp]['stat_max_time_waited_grid'])\n",
    "\n",
    "means_1 = []\n",
    "for i in range(len(max_time_grid[0])):\n",
    "    means_1.append(np.mean(max_time_grid[0][i]))\n",
    "means_2 = []\n",
    "for i in range(len(max_time_grid[1])):\n",
    "    means_2.append(np.mean(max_time_grid[1][i]))\n",
    "\n",
    "\n",
    "results = stats.ttest_ind(means_1, means_2)\n",
    "print(results)\n",
    "print(\"The max time waited for the first two simulations are: \" + str(np.mean(means_1)) + \" and \" + str(np.mean(means_2)))\n",
    "print(\"The p-value is: \" + str(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gini coefficient <a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=-5.385768304026473, pvalue=4.0596978338806996e-05, df=18.0)\n",
      "The average gini of time waited for the first two simulations are: 0.1342073960076311 and 0.14004576193543522\n",
      "The p-value is: 4.0596978338806996e-05\n",
      "The difference is significant\n"
     ]
    }
   ],
   "source": [
    "# Average Time Waited.\n",
    "#Averaged over epochs and intersections: the gini of each intersection, and average of all the ginis per epoch.\n",
    "all_ginis_time_waited = []\n",
    "for sim in results_to_use:\n",
    "    all_ginis_time_waited.append(\n",
    "        all_sim_results[sim]['stat_time_waited_gini'])\n",
    "\n",
    "results = stats.ttest_ind(all_ginis_time_waited[0], all_ginis_time_waited[1])\n",
    "print(results)\n",
    "#present the results nicely\n",
    "print(\"The average gini of time waited for the first two simulations are: \" + str(np.mean(all_ginis_time_waited[0])) + \" and \" + str(np.mean(all_ginis_time_waited[1])))\n",
    "print(\"The p-value is: \" + str(results[1]))\n",
    "if results[1] < 0.05:\n",
    "    print(\"The difference is significant\")\n",
    "else:\n",
    "    print(\"The difference is not significant\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=-5.385768304026473, pvalue=4.0596978338806996e-05, df=18.0)\n",
      "The average gini of satisfaction for the first two simulations are: 0.43450183023533107 and 0.4267730882164245\n",
      "The p-value is: 4.0596978338806996e-05\n",
      "The difference is significant\n"
     ]
    }
   ],
   "source": [
    "# Average Satisfaction.\n",
    "#Averaged over epochs and intersections: the gini of each intersection, and average of all the ginis per epoch.\n",
    "all_ginis_satisfaction = []\n",
    "for sim in results_to_use:\n",
    "    all_ginis_satisfaction.append(\n",
    "        all_sim_results[sim]['stat_satisfaction_gini'])\n",
    "\n",
    "results = stats.ttest_ind(all_ginis_time_waited[0], all_ginis_time_waited[1])\n",
    "print(results)\n",
    "#present the results nicely\n",
    "print(\"The average gini of satisfaction for the first two simulations are: \" + str(np.mean(all_ginis_satisfaction[0])) + \" and \" + str(np.mean(all_ginis_satisfaction[1])))\n",
    "print(\"The p-value is: \" + str(results[1]))\n",
    "if results[1] < 0.05:\n",
    "    print(\"The difference is significant\")\n",
    "else:\n",
    "    print(\"The difference is not significant\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trip Satisfaction <a id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=-4.831750046471582, pvalue=0.0001338026967753625, df=18.0)\n",
      "The average satisfactino for the first two simulations are: 0.15497821877692497 and 0.1566294076515055\n",
      "The p-value is: 0.0001338026967753625\n",
      "The difference is significant\n"
     ]
    }
   ],
   "source": [
    "all_satisfactions = []\n",
    "for sim in results_to_use:\n",
    "    all_satisfactions.append(\n",
    "        all_sim_results[sim]['stat_satisfaction_mean'])\n",
    "\n",
    "results = stats.ttest_ind(all_satisfactions[0], all_satisfactions[1])\n",
    "print(results)\n",
    "#present the results nicely\n",
    "print(\"The average satisfactino for the first two simulations are: \" + str(np.mean(all_satisfactions[0])) + \" and \" + str(np.mean(all_satisfactions[1])))\n",
    "print(\"The p-value is: \" + str(results[1]))\n",
    "if results[1] < 0.05:\n",
    "    print(\"The difference is significant\")\n",
    "else:\n",
    "    print(\"The difference is not significant\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Auction Reward <a id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection [ 0 ][ 0 ]\n",
      "TtestResult(statistic=-1.2878762175796052, pvalue=0.2141007031489009, df=18.0)\n",
      "The average auction reward for the first two simulations are: 0.04920416666666667 and 0.051151666666666665\n",
      "The p-value is: 0.2141007031489009\n",
      "Intersection [ 0 ][ 1 ]\n",
      "TtestResult(statistic=-1.7340523802142798, pvalue=0.10000203606109737, df=18.0)\n",
      "The average auction reward for the first two simulations are: 0.2450558333333333 and 0.2503558333333333\n",
      "The p-value is: 0.10000203606109737\n",
      "Intersection [ 0 ][ 2 ]\n",
      "TtestResult(statistic=-1.4250817349180562, pvalue=0.17124317379775145, df=18.0)\n",
      "The average auction reward for the first two simulations are: 0.04953416666666667 and 0.05115583333333333\n",
      "The p-value is: 0.17124317379775145\n",
      "Intersection [ 1 ][ 0 ]\n",
      "TtestResult(statistic=-3.48126312371585, pvalue=0.0026653144364041878, df=18.0)\n",
      "The average auction reward for the first two simulations are: 0.24761666666666665 and 0.26056749999999995\n",
      "The p-value is: 0.0026653144364041878\n",
      "Intersection [ 1 ][ 1 ]\n",
      "TtestResult(statistic=-1.4756529863413614, pvalue=0.15731603831685773, df=18.0)\n",
      "The average auction reward for the first two simulations are: 0.6468316666666667 and 0.6516925\n",
      "The p-value is: 0.15731603831685773\n",
      "Intersection [ 1 ][ 2 ]\n",
      "TtestResult(statistic=-1.3004795956972695, pvalue=0.20984157301755563, df=18.0)\n",
      "The average auction reward for the first two simulations are: 0.24701750000000003 and 0.2532766666666666\n",
      "The p-value is: 0.20984157301755563\n",
      "Intersection [ 2 ][ 0 ]\n",
      "TtestResult(statistic=-1.0461102214499276, pvalue=0.30935553674898897, df=18.0)\n",
      "The average auction reward for the first two simulations are: 0.048074166666666654 and 0.049555833333333334\n",
      "The p-value is: 0.30935553674898897\n",
      "Intersection [ 2 ][ 1 ]\n",
      "TtestResult(statistic=-0.6440199537184071, pvalue=0.5276843247060468, df=18.0)\n",
      "The average auction reward for the first two simulations are: 0.23964999999999997 and 0.24224166666666663\n",
      "The p-value is: 0.5276843247060468\n",
      "Intersection [ 2 ][ 2 ]\n",
      "TtestResult(statistic=-0.46427079427999823, pvalue=0.6480197120544304, df=18.0)\n",
      "The average auction reward for the first two simulations are: 0.04915749999999999 and 0.04963333333333333\n",
      "The p-value is: 0.6480197120544304\n"
     ]
    }
   ],
   "source": [
    "######Intersection based - Average Auction Reward######\n",
    "\n",
    "average_reward_intersection = []\n",
    "for exp in results_to_use:\n",
    "    average_reward_intersection.append(\n",
    "        all_sim_results[exp]['stat_average_auction_reward_per_intersection'])\n",
    "    \n",
    "for i in range(len(average_reward_intersection[0][0])):\n",
    "    for j in range(len(average_reward_intersection[0][0])):\n",
    "        print(\"Intersection [\", i, \"][\", j, \"]\")\n",
    "        results = stats.ttest_ind(average_reward_intersection[0][:,i,j], average_reward_intersection[1][:,i,j])\n",
    "        print(results)\n",
    "        print(\"The average auction reward for the first two simulations are: \" + str(np.mean(average_reward_intersection[0][:,i,j])) + \" and \" + str(np.mean(average_reward_intersection[1][:,i,j])))\n",
    "        print(\"The p-value is: \" + str(results[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
