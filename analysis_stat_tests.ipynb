{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Analysis Notebook\n",
    "\n",
    "This notebook can be used to calculate statistical metrics for the data produced. \n",
    "*** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup <a class=\"anchor\" id=\"0\"></a>\n",
    "This section imports all files and sets up the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all pacakges\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using results: ['Adaptive', 'Zero']\n"
     ]
    }
   ],
   "source": [
    "# Choose which results to use\n",
    "results_to_use = [\"Adaptive\", \"Zero\"]  # If this is empty, all files in the folder will be used\n",
    "WARMUP_EPOCHS = 0 # Number of epochs to ignore.\n",
    "sim_colours = ['blue', 'green', 'red', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "\n",
    "\n",
    "all_results = os.listdir('results/')\n",
    "\n",
    "if results_to_use == [\"\"]:\n",
    "    results_to_use = all_results\n",
    "# remove DS_Store from the list\n",
    "if '.DS_Store' in results_to_use:\n",
    "    results_to_use.remove('.DS_Store')\n",
    "    \n",
    "print(\"Using results: \" + str(results_to_use))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary. Each key is a set of results. \n",
    "\n",
    "all_sim_results = {}\n",
    "\n",
    "for sim in results_to_use:\n",
    "    all_sim_results[sim] = {}\n",
    "    all_sim_results[sim]['configuration'] = pd.read_csv(\n",
    "        'results/' + sim + '/configuration.txt', header=None, delimiter=\":\")\n",
    "    types_of_results = []\n",
    "    # Get all the filenames in the folder, excluding the extension\n",
    "    for file in os.listdir('results/' + sim + '/exported_data/'):\n",
    "        if file.endswith(\".npy\"):\n",
    "            types_of_results.append(file[:-4])\n",
    "    # Create a dictionary for each type of result\n",
    "    for result in types_of_results:\n",
    "        all_sim_results[sim][result] = np.load(\n",
    "            'results/' + sim + '/exported_data/' + result + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Adaptive\n",
      "                                    0                    1\n",
      "0                             command                  run\n",
      "1                       num_of_epochs                10000\n",
      "2                  num_of_simulations                  100\n",
      "3                           grid_size                    3\n",
      "4                      queue_capacity                   10\n",
      "5                     congestion_rate                 0.07\n",
      "6                       with_hotspots                 True\n",
      "7                           wage_time                   10\n",
      "8                      credit_balance                    5\n",
      "9           shared_auction_parameters                 None\n",
      "10  adaptive_auction_action_selection   e_greedy_exp_decay\n",
      "11               bid_calculation_rule               linear\n",
      "12       adaptive_auction_update_rule        simple_bandit\n",
      "13             auction_episode_length                   10\n",
      "14   action_selection_hyperparameters                 None\n",
      "15    adaptive_auction_discretization                   25\n",
      "16             only_winning_bid_moves                 None\n",
      "17           intersection_reward_type    mixed_metric_rank\n",
      "18                  inact_rank_weight                  0.5\n",
      "19                    bid_rank_weight                  0.5\n",
      "20                       all_cars_bid                 True\n",
      "21               shared_bid_generator                 None\n",
      "22                 bidders_proportion      [1, 0, 0, 0, 1]\n",
      "23       bidders_urgency_distribution             gaussian\n",
      "24                     results_folder     results/Adaptive\n",
      "25                         print_grid                 None\n",
      "26                         sweep_mode                 None\n",
      "27                            low_dpi                 None\n",
      "Results: Zero\n",
      "                                    0                   1\n",
      "0                             command                 run\n",
      "1                       num_of_epochs               10000\n",
      "2                  num_of_simulations                 100\n",
      "3                           grid_size                   3\n",
      "4                      queue_capacity                  10\n",
      "5                     congestion_rate                0.07\n",
      "6                       with_hotspots                True\n",
      "7                           wage_time                  10\n",
      "8                      credit_balance                   5\n",
      "9           shared_auction_parameters                None\n",
      "10  adaptive_auction_action_selection                zero\n",
      "11               bid_calculation_rule              linear\n",
      "12       adaptive_auction_update_rule       simple_bandit\n",
      "13             auction_episode_length                  10\n",
      "14   action_selection_hyperparameters                None\n",
      "15    adaptive_auction_discretization                  25\n",
      "16             only_winning_bid_moves                None\n",
      "17           intersection_reward_type   mixed_metric_rank\n",
      "18                  inact_rank_weight                 0.5\n",
      "19                    bid_rank_weight                 0.5\n",
      "20                       all_cars_bid                True\n",
      "21               shared_bid_generator                None\n",
      "22                 bidders_proportion     [1, 0, 0, 0, 1]\n",
      "23       bidders_urgency_distribution            gaussian\n",
      "24                     results_folder        results/Zero\n",
      "25                         print_grid                None\n",
      "26                         sweep_mode                None\n",
      "27                            low_dpi                None\n"
     ]
    }
   ],
   "source": [
    "# Show all the all_sim_results in the notebook\n",
    "for set_of_results in results_to_use:\n",
    "    print(\"Results: \" + set_of_results)\n",
    "    print(all_sim_results[set_of_results]['configuration'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Average Total Number of Trips  <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "This t-test compares the average total number of trips between the two experiments, for all sims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=14.941614323335866, pvalue=2.6868412356045133e-34, df=198.0)\n",
      "The average number of trips for the first two simulations are: 24888.73 and 24560.77\n",
      "the standard deviation of number of trips for the first two simulations are: 147.5366974687993 and 161.02458538993355\n",
      "in latex: $(M= 24888.73 , SD= 147.537 )$ $(M= 24560.77 , SD= 161.025 )$\n",
      "in latex: t( 198.0 )= 14.942 , p= 0.0\n",
      "The p-value is: 2.6868412356045133e-34\n",
      "The difference is significant\n"
     ]
    }
   ],
   "source": [
    "all_average_number_of_trips = []\n",
    "for sim in results_to_use:\n",
    "    all_average_number_of_trips.append(\n",
    "        all_sim_results[sim]['stat_num_of_trips_per_simulation'])\n",
    "\n",
    "results = stats.ttest_ind(all_average_number_of_trips[0], all_average_number_of_trips[1])\n",
    "print(results)\n",
    "#present the results nicely\n",
    "print(\"The average number of trips for the first two simulations are: \" + str(np.mean(all_average_number_of_trips[0])) + \" and \" + str(np.mean(all_average_number_of_trips[1])))\n",
    "print(\"the standard deviation of number of trips for the first two simulations are: \" + str(np.std(all_average_number_of_trips[0])) + \" and \" + str(np.std(all_average_number_of_trips[1])))\n",
    "mean_1 = round(np.mean(all_average_number_of_trips[0]),3)\n",
    "mean_2 = round(np.mean(all_average_number_of_trips[1]),3)\n",
    "std_1 = round(np.std(all_average_number_of_trips[0]),3)\n",
    "std_2 = round(np.std(all_average_number_of_trips[1]),3)\n",
    "print(\"in latex:\",\"$(M=\",mean_1,\", SD=\",std_1,\")$\",\"$(M=\",mean_2,\", SD=\",std_2,\")$\")\n",
    "print(\"in latex:\", \"t(\", results.df, \")=\", round(results[0], 3), \", p=\", round(results[1], 3))\n",
    "print(\"The p-value is: \" + str(results[1]))\n",
    "if results[1] < 0.05:\n",
    "    print(\"The difference is significant\")\n",
    "else:\n",
    "    print(\"The difference is not significant\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Average Congestion of Central Intersection\n",
    "\n",
    "This t-test is between the 2 experiments, for all sims. It does not process individual epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=-47.938938550178214, pvalue=6.4716605752490096e-111, df=198.0)\n",
      "The average congestion for the first two simulations are: 0.30803945 and 0.34397810000000006\n",
      "The p-value is: 6.4716605752490096e-111\n",
      "The difference is significant\n",
      "in latex: $(M= 0.308 ,\\ SD= 0.006 )$ $(M= 0.344 ,\\ SD= 0.005 )$\n",
      "in latex: $t( 198.0 )= -47.939 , p= 0.0 $\n"
     ]
    }
   ],
   "source": [
    "average_congestion_per_intersection = []\n",
    "for exp in results_to_use:\n",
    "    average_congestion_per_intersection.append(\n",
    "        all_sim_results[exp]['stat_average_congestion_per_intersection'])\n",
    "\n",
    "results = stats.ttest_ind(average_congestion_per_intersection[0][:,1,1], average_congestion_per_intersection[1][:,1,1])\n",
    "print(results)\n",
    "print(\"The average congestion for the first two simulations are: \" + str(np.mean(average_congestion_per_intersection[0][:,1,1])) + \" and \" + str(np.mean(average_congestion_per_intersection[1][:,1,1])))\n",
    "print(\"The p-value is: \" + str(results[1]))\n",
    "if results[1] < 0.05:\n",
    "    print(\"The difference is significant\")\n",
    "else:\n",
    "    print(\"The difference is not significant\")\n",
    "    \n",
    "mean_1 = round(np.mean(average_congestion_per_intersection[0][:,1,1]),3)\n",
    "mean_2 = round(np.mean(average_congestion_per_intersection[1][:,1,1]),3)\n",
    "std_1 = round(np.std(average_congestion_per_intersection[0][:,1,1]),3)\n",
    "std_2 = round(np.std(average_congestion_per_intersection[1][:,1,1]),3)\n",
    "print(\"in latex:\",\"$(M=\",mean_1,\",\\ SD=\",std_1,\")$\",\"$(M=\",mean_2,\",\\ SD=\",std_2,\")$\")\n",
    "print(\"in latex:\", \"$t(\", results.df, \")=\", round(results[0], 3), \", p=\", round(results[1], 3), \"$\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Average Time Waited <a id=\"3\"></a>\n",
    "\n",
    "agent, intersection and grid based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=-115.27669120235495, pvalue=1.8423776792123032e-183, df=198.0)\n",
      "The average time waited for the first two simulations are: 0.23983044444444446 and 0.60510575\n",
      "The p-value is: 1.8423776792123032e-183\n",
      "in latex: $(M= 0.24 ,\\ SD= 0.006 )$ $(M= 0.605 ,\\ SD= 0.031 )$\n",
      "in latex: $t( 198.0 )= -115.277 , p= 0.0 $\n",
      "TtestResult(statistic=-106.81933991370087, pvalue=5.179083985862078e-177, df=198.0)\n",
      "The max time waited for the first two simulations are: 0.6207537777777777 and 1.7008356666666669\n",
      "The p-value is: 5.179083985862078e-177\n",
      "in latex: $(M= 0.621 ,\\ SD= 0.014 )$ $(M= 1.701 ,\\ SD= 0.1 )$\n",
      "in latex: $t( 198.0 )= -106.819 ,\\ p= 0.0 $\n"
     ]
    }
   ],
   "source": [
    "######Agent based######\n",
    "# Average time\n",
    "average_time_agent = []\n",
    "for exp in results_to_use:\n",
    "    average_time_agent.append(\n",
    "        all_sim_results[exp]['stat_average_time_waited_per_simulation_agent'])\n",
    "    \n",
    "results = stats.ttest_ind(average_time_agent[0], average_time_agent[1])\n",
    "print(results)\n",
    "print(\"The average time waited for the first two simulations are: \" + str(np.mean(average_time_agent[0])) + \" and \" + str(np.mean(average_time_agent[1])))\n",
    "print(\"The p-value is: \" + str(results[1]))\n",
    "mean_1 = round(np.mean(average_time_agent[0]),3)\n",
    "mean_2 = round(np.mean(average_time_agent[1]),3)\n",
    "std_1 = round(np.std(average_time_agent[0]),3)\n",
    "std_2 = round(np.std(average_time_agent[1]),3)\n",
    "print(\"in latex:\",\"$(M=\",mean_1,\",\\ SD=\",std_1,\")$\",\"$(M=\",mean_2,\",\\ SD=\",std_2,\")$\")\n",
    "print(\"in latex:\", \"$t(\", results.df, \")=\", round(results[0], 3), \", p=\", round(results[1], 3), \"$\")\n",
    "\n",
    "# Max time\n",
    "max_time_agent = []\n",
    "for exp in results_to_use:\n",
    "    max_time_agent.append(\n",
    "        all_sim_results[exp]['stat_max_time_waited_per_simulation_agent'])\n",
    "    \n",
    "results = stats.ttest_ind(max_time_agent[0], max_time_agent[1])\n",
    "print(results)\n",
    "print(\"The max time waited for the first two simulations are: \" + str(np.mean(max_time_agent[0])) + \" and \" + str(np.mean(max_time_agent[1])))\n",
    "print(\"The p-value is: \" + str(results[1]))\n",
    "mean_1 = round(np.mean(max_time_agent[0]),3)\n",
    "mean_2 = round(np.mean(max_time_agent[1]),3)\n",
    "std_1 = round(np.std(max_time_agent[0]),3)\n",
    "std_2 = round(np.std(max_time_agent[1]),3)\n",
    "print(\"in latex:\",\"$(M=\",mean_1,\",\\ SD=\",std_1,\")$\",\"$(M=\",mean_2,\",\\ SD=\",std_2,\")$\")\n",
    "print(\"in latex:\", \"$t(\", results.df, \")=\", round(results[0], 3), \",\\ p=\", round(results[1], 3), \"$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection [ 0 ][ 0 ]:  Adaptive $(M= 0.027 ,\\ SD= 0.005 )$ , Zero $(M= 0.029 ,\\ SD= 0.004 ),\\ t( 198 )= -2.82 ,\\ p= 0.005 $\n",
      "Intersection [ 0 ][ 1 ]:  Adaptive $(M= 0.233 ,\\ SD= 0.02 )$ , Zero $(M= 0.649 ,\\ SD= 0.087 ),\\ t( 198 )= -46.22 ,\\ p= 0.0 $\n",
      "Intersection [ 0 ][ 2 ]:  Adaptive $(M= 0.027 ,\\ SD= 0.005 )$ , Zero $(M= 0.029 ,\\ SD= 0.004 ),\\ t( 198 )= -2.94 ,\\ p= 0.004 $\n",
      "Intersection [ 1 ][ 0 ]:  Adaptive $(M= 0.342 ,\\ SD= 0.038 )$ , Zero $(M= 0.699 ,\\ SD= 0.077 ),\\ t( 198 )= -41.31 ,\\ p= 0.0 $\n",
      "Intersection [ 1 ][ 1 ]:  Adaptive $(M= 0.906 ,\\ SD= 0.05 )$ , Zero $(M= 2.669 ,\\ SD= 0.144 ),\\ t( 198 )= -115.15 ,\\ p= 0.0 $\n",
      "Intersection [ 1 ][ 2 ]:  Adaptive $(M= 0.284 ,\\ SD= 0.027 )$ , Zero $(M= 0.678 ,\\ SD= 0.091 ),\\ t( 198 )= -41.32 ,\\ p= 0.0 $\n",
      "Intersection [ 2 ][ 0 ]:  Adaptive $(M= 0.026 ,\\ SD= 0.006 )$ , Zero $(M= 0.027 ,\\ SD= 0.004 ),\\ t( 198 )= -1.91 ,\\ p= 0.058 $\n",
      "Intersection [ 2 ][ 1 ]:  Adaptive $(M= 0.288 ,\\ SD= 0.035 )$ , Zero $(M= 0.639 ,\\ SD= 0.076 ),\\ t( 198 )= -41.68 ,\\ p= 0.0 $\n",
      "Intersection [ 2 ][ 2 ]:  Adaptive $(M= 0.025 ,\\ SD= 0.006 )$ , Zero $(M= 0.028 ,\\ SD= 0.004 ),\\ t( 198 )= -3.38 ,\\ p= 0.001 $\n"
     ]
    }
   ],
   "source": [
    "######Intersection based - Average Time######\n",
    "\n",
    "average_time_intersection = []\n",
    "for exp in results_to_use:\n",
    "    average_time_intersection.append(\n",
    "        all_sim_results[exp]['stat_average_time_waited_per_intersection'])\n",
    "    \n",
    "for i in range(len(average_time_intersection[0][0])):\n",
    "    for j in range(len(average_time_intersection[0][0])):\n",
    "        # print(\"Intersection [\", i, \"][\", j, \"]\")\n",
    "        results = stats.ttest_ind(average_time_intersection[0][:,i,j], average_time_intersection[1][:,i,j])\n",
    "        # print(results)\n",
    "        # print(\"The average time waited for the first two simulations are: \" + str(np.mean(average_time_intersection[0][:,i,j])) + \" and \" + str(np.mean(average_time_intersection[1][:,i,j])))\n",
    "        # print(\"The p-value is: \" + str(results[1]))\n",
    "        mean_1 = round(np.mean(average_time_intersection[0][:,i,j]),3)\n",
    "        mean_2 = round(np.mean(average_time_intersection[1][:,i,j]),3)\n",
    "        std_1 = round(np.std(average_time_intersection[0][:,i,j]),3)\n",
    "        std_2 = round(np.std(average_time_intersection[1][:,i,j]),3)\n",
    "        print(\"Intersection [\", i, \"][\", j, \"]: \", \"Adaptive $(M=\",mean_1,\",\\ SD=\",std_1,\")$\",\", Zero $(M=\",mean_2,\",\\ SD=\",std_2,\"),\\ t(\", int(results.df), \")=\", round(results[0], 2), \",\\ p=\", round(results[1], 3), \"$\")\n",
    "        # print(\"in latex:\",\"$(M=\",mean_1,\",\\ SD=\",std_1,\")$\",\"$(M=\",mean_2,\",\\ SD=\",std_2,\")$\")\n",
    "        # print(\"in latex:\", \"$t(\", results.df, \")=\", round(results[0], 3), \",\\ p=\", round(results[1], 3), \"$\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection [ 0 ][ 0 ]:  Adaptive $(M= 0.095 ,\\ SD= 0.014 )$ , Zero $(M= 0.105 ,\\ SD= 0.012 ),\\ t( 198 )= -5.36 ,\\ p= 0.0 $\n",
      "Intersection [ 0 ][ 1 ]:  Adaptive $(M= 0.651 ,\\ SD= 0.049 )$ , Zero $(M= 1.917 ,\\ SD= 0.272 ),\\ t( 198 )= -45.63 ,\\ p= 0.0 $\n",
      "Intersection [ 0 ][ 2 ]:  Adaptive $(M= 0.093 ,\\ SD= 0.014 )$ , Zero $(M= 0.104 ,\\ SD= 0.013 ),\\ t( 198 )= -5.56 ,\\ p= 0.0 $\n",
      "Intersection [ 1 ][ 0 ]:  Adaptive $(M= 0.914 ,\\ SD= 0.095 )$ , Zero $(M= 2.061 ,\\ SD= 0.247 ),\\ t( 198 )= -43.12 ,\\ p= 0.0 $\n",
      "Intersection [ 1 ][ 1 ]:  Adaptive $(M= 2.09 ,\\ SD= 0.122 )$ , Zero $(M= 7.016 ,\\ SD= 0.462 ),\\ t( 198 )= -102.5 ,\\ p= 0.0 $\n",
      "Intersection [ 1 ][ 2 ]:  Adaptive $(M= 0.776 ,\\ SD= 0.066 )$ , Zero $(M= 2.019 ,\\ SD= 0.296 ),\\ t( 198 )= -40.82 ,\\ p= 0.0 $\n",
      "Intersection [ 2 ][ 0 ]:  Adaptive $(M= 0.09 ,\\ SD= 0.016 )$ , Zero $(M= 0.099 ,\\ SD= 0.012 ),\\ t( 198 )= -4.28 ,\\ p= 0.0 $\n",
      "Intersection [ 2 ][ 1 ]:  Adaptive $(M= 0.788 ,\\ SD= 0.088 )$ , Zero $(M= 1.886 ,\\ SD= 0.239 ),\\ t( 198 )= -42.91 ,\\ p= 0.0 $\n",
      "Intersection [ 2 ][ 2 ]:  Adaptive $(M= 0.089 ,\\ SD= 0.015 )$ , Zero $(M= 0.101 ,\\ SD= 0.013 ),\\ t( 198 )= -5.94 ,\\ p= 0.0 $\n"
     ]
    }
   ],
   "source": [
    "######Intersection based - Max Time######\n",
    "\n",
    "max_time_intersection = []\n",
    "for exp in results_to_use:\n",
    "    max_time_intersection.append(\n",
    "        all_sim_results[exp]['stat_max_time_waited_per_intersection'])\n",
    "    \n",
    "for i in range(len(max_time_intersection[0][0])):\n",
    "    for j in range(len(max_time_intersection[0][0])):\n",
    "        # print(\"Intersection [\", i, \"][\", j, \"]\")\n",
    "        results = stats.ttest_ind(max_time_intersection[0][:,i,j], max_time_intersection[1][:,i,j])\n",
    "        mean_1 = round(np.mean(max_time_intersection[0][:,i,j]),3)\n",
    "        mean_2 = round(np.mean(max_time_intersection[1][:,i,j]),3)\n",
    "        std_1 = round(np.std(max_time_intersection[0][:,i,j]),3)\n",
    "        std_2 = round(np.std(max_time_intersection[1][:,i,j]),3)\n",
    "        print(\"Intersection [\", i, \"][\", j, \"]: \", \"Adaptive $(M=\",mean_1,\",\\ SD=\",std_1,\")$\",\", Zero $(M=\",mean_2,\",\\ SD=\",std_2,\"),\\ t(\", int(results.df), \")=\", round(results[0], 2), \",\\ p=\", round(results[1], 3), \"$\")\n",
    "        # print(results)\n",
    "        # print(\"The average time waited for the first two simulations are: \" + str(np.mean(max_time_intersection[0][:,i,j])) + \" and \" + str(np.mean(max_time_intersection[1][:,i,j])))\n",
    "        # print(\"The p-value is: \" + str(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE TIME WAITED\n",
      "[[0.0271565  0.233166   0.0268695 ]\n",
      " [0.34171025 0.906314   0.28369175]\n",
      " [0.02603975 0.288191   0.02533525]]\n",
      "TtestResult(statistic=-115.27669120235494, pvalue=1.842377679212408e-183, df=198.0)\n",
      "The average time waited for the first two simulations are: 0.23983044444444446 and 0.60510575\n",
      "The p-value is: 1.842377679212408e-183\n",
      "in latex: $(M= 0.24 ,\\ SD= 0.006 )$ $(M= 0.605 ,\\ SD= 0.031 )$\n",
      "in latex: $t( 198.0 )= -115.277 , p= 0.0 $\n"
     ]
    }
   ],
   "source": [
    "###### Grid based######\n",
    "\n",
    "# Average time\n",
    "print(\"AVERAGE TIME WAITED\")\n",
    "average_time_grid = []\n",
    "for exp in results_to_use:\n",
    "    average_time_grid.append(\n",
    "        all_sim_results[exp]['stat_average_time_waited_grid'])\n",
    "\n",
    "means_1 = []\n",
    "for i in range(len(average_time_grid[0])):\n",
    "    means_1.append(np.mean(average_time_grid[0][i]))\n",
    "means_2 = []\n",
    "for i in range(len(average_time_grid[1])):\n",
    "    means_2.append(np.mean(average_time_grid[1][i]))\n",
    "\n",
    "print(np.mean(average_time_grid[0], axis=0))\n",
    "# print(\"mean = \", np.mean(np.mean(average_time_grid[0], axis=0)))\n",
    "# print(\"std = \", np.std(np.mean(average_time_grid[0], axis=0)))\n",
    "\n",
    "results = stats.ttest_ind(means_1, means_2)\n",
    "print(results)\n",
    "print(\"The average time waited for the first two simulations are: \" +\n",
    "      str(np.mean(means_1)) + \" and \" + str(np.mean(means_2)))\n",
    "print(\"The p-value is: \" + str(results[1]))\n",
    "mean_1 = round(np.mean(means_1), 3)\n",
    "mean_2 = round(np.mean(means_2), 3)\n",
    "std_1 = round(np.std(means_1), 3)\n",
    "std_2 = round(np.std(means_2), 3)\n",
    "print(\"in latex:\", \"$(M=\", mean_1, \",\\ SD=\", std_1, \")$\",\n",
    "      \"$(M=\", mean_2, \",\\ SD=\", std_2, \")$\")\n",
    "print(\"in latex:\", \"$t(\", results.df, \")=\", round(\n",
    "    results[0], 3), \", p=\", round(results[1], 3), \"$\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Max time\n",
    "# print(\"MAX TIME WAITED\")\n",
    "# max_time_grid = []\n",
    "# for exp in results_to_use:\n",
    "#     max_time_grid.append(\n",
    "#         all_sim_results[exp]['stat_max_time_waited_grid'])\n",
    "\n",
    "# means_1 = []\n",
    "# for i in range(len(max_time_grid[0])):\n",
    "#     means_1.append(np.mean(max_time_grid[0][i]))\n",
    "# means_2 = []\n",
    "# for i in range(len(max_time_grid[1])):\n",
    "#     means_2.append(np.mean(max_time_grid[1][i]))\n",
    "\n",
    "\n",
    "# results = stats.ttest_ind(means_1, means_2)\n",
    "# print(results)\n",
    "# print(\"The max time waited for the first two simulations are: \" +\n",
    "#       str(np.mean(means_1)) + \" and \" + str(np.mean(means_2)))\n",
    "# print(\"The p-value is: \" + str(results[1]))\n",
    "# mean_1 = round(np.mean(means_1), 3)\n",
    "# mean_2 = round(np.mean(means_2), 3)\n",
    "# std_1 = round(np.std(means_1), 3)\n",
    "# std_2 = round(np.std(means_2), 3)\n",
    "# print(\"in latex:\", \"$(M=\", mean_1, \",\\ SD=\", std_1, \")$\",\n",
    "#       \"$(M=\", mean_2, \",\\ SD=\", std_2, \")$\")\n",
    "# print(\"in latex:\", \"$t(\", results.df, \")=\", round(\n",
    "#     results[0], 3), \", p=\", round(results[1], 3), \"$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gini coefficient <a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=33.412419014122115, pvalue=2.5368267435980213e-83, df=198.0)\n",
      "The average gini of time waited for the first two simulations are: 0.14131020591026344 and 0.1314799973220571\n",
      "The p-value is: 2.5368267435980213e-83\n",
      "in latex: $(M= 0.141 ,\\ SD= 0.002 )$ $(M= 0.131 ,\\ SD= 0.002 )$\n",
      "in latex: $t( 198.0 )= 33.412 , p= 0.0 $\n"
     ]
    }
   ],
   "source": [
    "# Average Time Waited.\n",
    "#Averaged over epochs and intersections: the gini of each intersection, and average of all the ginis per epoch.\n",
    "all_ginis_time_waited = []\n",
    "for sim in results_to_use:\n",
    "    all_ginis_time_waited.append(\n",
    "        all_sim_results[sim]['stat_time_waited_gini'])\n",
    "\n",
    "results = stats.ttest_ind(all_ginis_time_waited[0], all_ginis_time_waited[1])\n",
    "print(results)\n",
    "# Present the results nicely\n",
    "print(\"The average gini of time waited for the first two simulations are: \" + str(np.mean(all_ginis_time_waited[0])) + \" and \" + str(np.mean(all_ginis_time_waited[1])))\n",
    "print(\"The p-value is: \" + str(results[1]))\n",
    "mean_1 = round(np.mean(all_ginis_time_waited[0]),3)\n",
    "mean_2 = round(np.mean(all_ginis_time_waited[1]),3)\n",
    "std_1 = round(np.std(all_ginis_time_waited[0]),3)\n",
    "std_2 = round(np.std(all_ginis_time_waited[1]),3)\n",
    "print(\"in latex:\",\"$(M=\",mean_1,\",\\ SD=\",std_1,\")$\",\"$(M=\",mean_2,\",\\ SD=\",std_2,\")$\")\n",
    "print(\"in latex:\", \"$t(\", results.df, \")=\", round(results[0], 3), \", p=\", round(results[1], 3), \"$\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=33.412419014122115, pvalue=2.5368267435980213e-83, df=198.0)\n",
      "The average gini of satisfaction for the first two simulations are: 0.4254760946572833 and 0.444973624021619\n",
      "The p-value is: 2.5368267435980213e-83\n",
      "in latex: $(M= 0.425 ,\\ SD= 0.002 )$ $(M= 0.445 ,\\ SD= 0.002 )$\n",
      "in latex: $t( 198.0 )= 33.412 , p= 0.0 $\n"
     ]
    }
   ],
   "source": [
    "# Average Satisfaction.\n",
    "#Averaged over epochs and intersections: the gini of each intersection, and average of all the ginis per epoch.\n",
    "all_ginis_satisfaction = []\n",
    "for sim in results_to_use:\n",
    "    all_ginis_satisfaction.append(\n",
    "        all_sim_results[sim]['stat_satisfaction_gini'])\n",
    "\n",
    "results = stats.ttest_ind(all_ginis_time_waited[0], all_ginis_time_waited[1])\n",
    "print(results)\n",
    "#present the results nicely\n",
    "print(\"The average gini of satisfaction for the first two simulations are: \" + str(np.mean(all_ginis_satisfaction[0])) + \" and \" + str(np.mean(all_ginis_satisfaction[1])))\n",
    "print(\"The p-value is: \" + str(results[1]))\n",
    "mean_1 = round(np.mean(all_ginis_satisfaction[0]),3)\n",
    "mean_2 = round(np.mean(all_ginis_satisfaction[1]),3)\n",
    "std_1 = round(np.std(all_ginis_satisfaction[0]),3)\n",
    "std_2 = round(np.std(all_ginis_satisfaction[1]),3)\n",
    "print(\"in latex:\",\"$(M=\",mean_1,\",\\ SD=\",std_1,\")$\",\"$(M=\",mean_2,\",\\ SD=\",std_2,\")$\")\n",
    "print(\"in latex:\", \"$t(\", results.df, \")=\", round(results[0], 3), \", p=\", round(results[1], 3), \"$\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trip Satisfaction <a id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=-26.461828416528476, pvalue=6.18137240128031e-67, df=198.0)\n",
      "The average satisfaction for the first two simulations are: 0.15633658081673904 and 0.1599203439257372\n",
      "The p-value is: 6.18137240128031e-67\n",
      "in latex: $(M= 0.156 ,\\ SD= 0.001 )$ $(M= 0.16 ,\\ SD= 0.001 )$\n",
      "in latex: $t( 198.0 )= -26.462 , p= 0.0 $\n"
     ]
    }
   ],
   "source": [
    "all_satisfactions = []\n",
    "for sim in results_to_use:\n",
    "    all_satisfactions.append(\n",
    "        all_sim_results[sim]['stat_satisfaction_mean'])\n",
    "\n",
    "results = stats.ttest_ind(all_satisfactions[0], all_satisfactions[1])\n",
    "print(results)\n",
    "#present the results nicely\n",
    "print(\"The average satisfaction for the first two simulations are: \" + str(np.mean(all_satisfactions[0])) + \" and \" + str(np.mean(all_satisfactions[1])))\n",
    "print(\"The p-value is: \" + str(results[1]))\n",
    "mean_1 = round(np.mean(all_satisfactions[0]),3)\n",
    "mean_2 = round(np.mean(all_satisfactions[1]),3)\n",
    "std_1 = round(np.std(all_satisfactions[0]),3)\n",
    "std_2 = round(np.std(all_satisfactions[1]),3)\n",
    "print(\"in latex:\",\"$(M=\",mean_1,\",\\ SD=\",std_1,\")$\",\"$(M=\",mean_2,\",\\ SD=\",std_2,\")$\")\n",
    "print(\"in latex:\", \"$t(\", results.df, \")=\", round(results[0], 3), \", p=\", round(results[1], 3), \"$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Auction Reward <a id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection [ 0 ][ 0 ]\n",
      "TtestResult(statistic=10.860884846980843, pvalue=7.410933872025321e-22, df=198.0)\n",
      "The average auction reward for the first two simulations are: 0.05166941666666667 and 0.046528\n",
      "The p-value is: 7.410933872025321e-22\n",
      "Intersection [ 0 ][ 1 ]\n",
      "TtestResult(statistic=17.44435368049987, pvalue=6.831514697775527e-42, df=198.0)\n",
      "The average auction reward for the first two simulations are: 0.25268666666666667 and 0.23226274999999993\n",
      "The p-value is: 6.831514697775527e-42\n",
      "Intersection [ 0 ][ 2 ]\n",
      "TtestResult(statistic=13.13635383548528, pvalue=9.31059008586125e-29, df=198.0)\n",
      "The average auction reward for the first two simulations are: 0.05181191666666665 and 0.046210999999999995\n",
      "The p-value is: 9.31059008586125e-29\n",
      "Intersection [ 1 ][ 0 ]\n",
      "TtestResult(statistic=28.620625885612217, pvalue=2.745988571859279e-72, df=198.0)\n",
      "The average auction reward for the first two simulations are: 0.2633661666666666 and 0.23000074999999995\n",
      "The p-value is: 2.745988571859279e-72\n",
      "Intersection [ 1 ][ 1 ]\n",
      "TtestResult(statistic=12.128322467001288, pvalue=1.119768391301334e-25, df=198.0)\n",
      "The average auction reward for the first two simulations are: 0.651564 and 0.6377701666666666\n",
      "The p-value is: 1.119768391301334e-25\n",
      "Intersection [ 1 ][ 2 ]\n",
      "TtestResult(statistic=20.55744873025864, pvalue=5.2044296930970764e-51, df=198.0)\n",
      "The average auction reward for the first two simulations are: 0.25303249999999994 and 0.22860574999999997\n",
      "The p-value is: 5.2044296930970764e-51\n",
      "Intersection [ 2 ][ 0 ]\n",
      "TtestResult(statistic=11.157656845945542, pvalue=9.608648187716604e-23, df=198.0)\n",
      "The average auction reward for the first two simulations are: 0.04919033333333333 and 0.04418950000000001\n",
      "The p-value is: 9.608648187716604e-23\n",
      "Intersection [ 2 ][ 1 ]\n",
      "TtestResult(statistic=19.0843401279958, pvalue=9.460177631496165e-47, df=198.0)\n",
      "The average auction reward for the first two simulations are: 0.24420124999999995 and 0.2204958333333333\n",
      "The p-value is: 9.460177631496165e-47\n",
      "Intersection [ 2 ][ 2 ]\n",
      "TtestResult(statistic=10.448342558297286, pvalue=1.240217464700695e-20, df=198.0)\n",
      "The average auction reward for the first two simulations are: 0.05035499999999999 and 0.045572416666666664\n",
      "The p-value is: 1.240217464700695e-20\n"
     ]
    }
   ],
   "source": [
    "######Intersection based - Average Auction Reward######\n",
    "\n",
    "average_reward_intersection = []\n",
    "for exp in results_to_use:\n",
    "    average_reward_intersection.append(\n",
    "        all_sim_results[exp]['stat_average_auction_reward_per_intersection'])\n",
    "    \n",
    "for i in range(len(average_reward_intersection[0][0])):\n",
    "    for j in range(len(average_reward_intersection[0][0])):\n",
    "        print(\"Intersection [\", i, \"][\", j, \"]\")\n",
    "        results = stats.ttest_ind(average_reward_intersection[0][:,i,j], average_reward_intersection[1][:,i,j])\n",
    "        print(results)\n",
    "        print(\"The average auction reward for the first two simulations are: \" + str(np.mean(average_reward_intersection[0][:,i,j])) + \" and \" + str(np.mean(average_reward_intersection[1][:,i,j])))\n",
    "        print(\"The p-value is: \" + str(results[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
